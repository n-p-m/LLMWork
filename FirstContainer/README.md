First Container-> Quantized Llama2 which runs on CPU.
Goal was to learn how to develop containers for LLM. Need to resolve below issues.

Need to incorporate:
1. pipenv for piplock
2. Token length issue.
Number of tokens (630) exceeded maximum context length (512).
Number of tokens (631) exceeded maximum context length (512).
Number of tokens (632) exceeded maximum context length (512).
Number of tokens (633) exceeded maximum context length (512).
Number of tokens (634) exceeded maximum context length (512).
2. Its running on CPU, should implement with GPU
3. Use RAG instead of converstional agent. 
