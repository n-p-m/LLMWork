{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_core\n",
      "  Obtaining dependency information for langchain_core from https://files.pythonhosted.org/packages/25/d9/c22eb2606e9c77eeb99099dfc9a114e848386227e123ac4fb14a0ed954c6/langchain_core-0.1.18-py3-none-any.whl.metadata\n",
      "  Using cached langchain_core-0.1.18-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from langchain_core) (6.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from langchain_core) (3.5.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1,>=0.0.83 (from langchain_core)\n",
      "  Obtaining dependency information for langsmith<0.1,>=0.0.83 from https://files.pythonhosted.org/packages/67/a0/a43239e2a0fa12867c66ef3f40537afb1d23c2830fef8bb624f578bf35e2/langsmith-0.0.86-py3-none-any.whl.metadata\n",
      "  Using cached langsmith-0.0.86-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain_core)\n",
      "  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from langchain_core) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from langchain_core) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from langchain_core) (8.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain_core) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain_core) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain_core) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_core) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_core) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_core) (2023.11.17)\n",
      "Using cached langchain_core-0.1.18-py3-none-any.whl (237 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.0.86-py3-none-any.whl (54 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: packaging, jsonpatch, langsmith, langchain_core\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed jsonpatch-1.33 langchain_core-0.1.18 langsmith-0.0.86 packaging-23.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain\n",
    "pip install --upgrade langchain_core\n",
    "pip install --upgrade sqlalchemy\n",
    "# pip install prompts_chat_pdf Not USING THIS\n",
    "pip install -q pypdf\n",
    "pip install -q sentence-transformers\n",
    "pip install -q faiss-cpu\n",
    "pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.llms import CTransformers\n",
    "# import chainlit as cl\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# from prompts_chat_pdf import chat_prompt, CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PDFChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data_path = os.path.join('data')\n",
    "        self.db_faiss_path = os.path.join('vectordb', 'db_faiss')\n",
    "        #self.chat_prompt = PromptTemplate(template=chat_prompt, input_variables=['context', 'question'])\n",
    "        #self.CONDENSE_QUESTION_PROMPT=CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "    def create_vector_db(self):\n",
    "\n",
    "        '''function to create vector db provided the pdf files'''\n",
    "\n",
    "        loader = DirectoryLoader(self.data_path,\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,\n",
    "                                                   chunk_overlap=50)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "\n",
    "        db = FAISS.from_documents(texts, embeddings)\n",
    "        db.save_local(self.db_faiss_path)\n",
    "\n",
    "    def load_llm(self):\n",
    "        # Load the locally downloaded model here\n",
    "        llm = CTransformers(\n",
    "            model=\"llama-2-7b.ggmlv3.q4_1.bin\",\n",
    "            model_type=\"llama\",\n",
    "            max_new_tokens=2000,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        return llm\n",
    "\n",
    "    def conversational_chain(self):\n",
    "\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                           model_kwargs={'device': 'cpu'})\n",
    "        db = FAISS.load_local(self.db_faiss_path, embeddings)\n",
    "        # initializing the conversational chain\n",
    "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        conversational_chain = ConversationalRetrievalChain.from_llm( llm=self.load_llm(),\n",
    "                                                                      retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                                                      verbose=True,\n",
    "                                                                      memory=memory\n",
    "                                                                      )\n",
    "\n",
    "        return conversational_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `arun` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use ainvoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "fit than complex ones.\n",
      " A simple model  in this context is a model where the distribution of parameter values\n",
      "has less entropy (or a model with fewer para meters, as you saw in the previous sec-\n",
      "tion). Thus a common way to mitigate overfi tting is to put constraints on the complex-\n",
      "ity of a network by forcing its weights to take only small values, which makes the\n",
      "\n",
      "the model on the validation data. In essence, this tuning is a form of learning : a search\n",
      "for a good configuration in some parameter space. As a result, tuning the configura-\n",
      "tion of the model based on its performance on the validation set can quickly result in\n",
      "overfitting to the validation set , even though your model is never directly trained on it.\n",
      "\n",
      "began to overfit . That is, their performance on neve r-before-seen data started stalling\n",
      "(or worsening) compared to their perfor mance on the training data—which always\n",
      "improves as training progresses.\n",
      " In machine learning, the goal is to achieve models that generalize —that perform\n",
      "well on never-before-seen data—and overfi tting is the central obstacle. You can only\n",
      "\n",
      "Question: what is over fitting?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An ML algorithm gets \"tuned\" in such a way as to make it fit better than necessary, i.e., not generalize as well on out of sample data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intialize_chain():\n",
    "    bot = PDFChatBot()\n",
    "    bot.create_vector_db()\n",
    "    conversational_chain = bot.conversational_chain()\n",
    "    return conversational_chain\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "chain = intialize_chain()\n",
    "\n",
    "await chain.arun(\"what is over fitting?\")\n",
    "# @cl.on_chat_start\n",
    "# async def start():\n",
    "#     msg = cl.Message(content=\"Starting the bot...\")\n",
    "#     await msg.send()\n",
    "#     msg.content = \"Welcome to Data Science Interview Prep Bot. Ask me your question?\"\n",
    "#     await msg.update()\n",
    "\n",
    "#     cl.user_session.set(\"chain\", chain)\n",
    "\n",
    "\n",
    "# @cl.on_message\n",
    "# async def main(message):\n",
    "#     chain = cl.user_session.get(\"chain\")\n",
    "#     cb = cl.AsyncLangchainCallbackHandler(\n",
    "#         stream_final_answer=False, answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "#     )\n",
    "#     cb.answer_reached = True\n",
    "#     res = await chain.acall({\"question\": message, \"chat_history\": chat_history}, callbacks=[cb])\n",
    "#     answer = res[\"answer\"]\n",
    "#     chat_history.append(answer)\n",
    "#     await cl.Message(content=answer).send()\n",
    "\n",
    "#     # while(True):\n",
    "#     #     query = input('User: ')\n",
    "#     #     response = conversational_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "#     #     chat_history.append(response[\"answer\"])  # Append the answer to chat history\n",
    "#     #     print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
